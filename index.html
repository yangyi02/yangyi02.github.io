<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZB7Y76KNGM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-ZB7Y76KNGM');
  </script>
  <title>Yi Yang's Homepage</title>
  <meta charset="utf-8">
  <link rel="stylesheet" type="text/css" href="style.css" media="all">
  <link rel='stylesheet' type='text/css' href='http://fonts.googleapis.com/css?family=Lato:300,400,700'>
</head>

<body>
  <div class="section">
    <div>
      <h1><span itemprop="name">Yi Yang (杨亿)</span></h1>
    </div>
    <p>I believe in Jesus Christ. 我信耶稣基督。</p>
    <p>I am currently a research scientist at <a href="http://www.deepmind.com/">Google DeepMind</a>. My research interests are self-supervised visual representation learning of motion, depth and segmentation for objects.</p>
    <p>Previously, I was a research scientist at <a href="http://research.baidu.com/">Baidu Research</a> from 2013 to 2018. 
      I obtained my Ph.D. degree in <a href="http://www.ics.uci.edu">Computer Science</a> at <a href="http://www.uci.edu">UC Irvine</a> in 2013. 
      I had summer internships at <a href="http://www.google.com/">Google</a> and <a href="http://research.microsoft.com/en-us/">Microsoft Research</a>. 
      I obtained my master degree in <a href="https://ieda.ust.hk/eng/index.php">Industrial Engineering</a> at <a href="http://www.ust.hk">Hong Kong University of Science and Technology</a> in 2008, and bachelor degree in <a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html">Automation</a> at <a href="http://www.tsinghua.edu.cn">Tsinghua University</a> in 2006.</p>
  </div>

  <div class="section">
    <h2>Publications</h2>

    <div class="publication">
      <img src="research/tapir/icon.png" alt="" />
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</a></strong><br>
      Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, Andrew Zisserman<br>
      <em>ICCV 2023</em><br>
      <p class="link"><a href="https://arxiv.org/pdf/2306.08637.pdf" class="first">Paper</a> &bull; <a href="https://deepmind-tapir.github.io/">Project Page</a> &bull; <a href="https://github.com/deepmind/tapnet">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/perception_test/icon.png" alt="" style="height: 162px"/>
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">Perception Test: A Diagnostic Benchmark for Multimodal Video Models</a></strong><br>
        Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, João Carreira<br>
      <em>NeurIPS 2023</em><br>
      <p class="link"><a href="https://arxiv.org/pdf/2305.13786.pdf" class="first">Paper</a> &bull; <a href="https://github.com/deepmind/perception_test">Dataset</a> &bull; <a href="https://www.deepmind.com/blog/measuring-perception-in-ai-models">Blog Post</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/tap_vid/icon.png" alt="" />
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">TAP-Vid: A Benchmark for Tracking Any Point in a Video</a></strong><br>
      Carl Doersch, Ankush Gupta, Larisa Markeeva, Adrià Recasens, Lucas Smaira, Yusuf Aytar, João Carreira, Andrew Zisserman, Yi Yang<br>
      <em>NeurIPS 2022</em><br>
      <p class="link"><a href="https://openreview.net/pdf?id=Zmosb2KfzYd" class="first">Paper</a> &bull; <a href="https://github.com/deepmind/tapnet">Dataset</a> &bull; <a href="https://twitter.com/GoogleDeepMind/status/1590029887213273089">Twitter</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/lip_sync/icon.png" alt="" />
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">Large-Scale Multilingual Audio Video Dubbing</a></strong><br>
      Yi Yang, Brendan Shillingford, Yannis Assael, Miaosen Wang, Wendi Liu, Yutian Chen, Yu Zhang, Eren Sezener, Luis C. Cobo, Misha Denil, Yusuf Aytar, Nando de Freitas<br>
      <em>Arxiv 2020</em><br>
      <p class="link"><a href="https://arxiv.org/pdf/2011.03530.pdf" class="first">Paper</a> &bull; <a href="https://www.youtube.com/channel/UC6CIb_eaKEsKE213TVEdYFw">Demo Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/refined_pose_dataset/icon.png" alt="" />
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">A Refined 3D Pose Dataset of Fine-Grained Object Categories</a></strong><br>
      Yaming Wang, Xiao Tan, Yi Yang, Ziyu Li, Xiao Liu, Feng Zhou, Larry S. Davis<br>
      <em>ICCV 2019 Workshop</em><br>
      <p class="link"><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Wang_A_Refined_3D_Pose_Dataset_for_Fine-Grained_Object_Categories_ICCVW_2019_paper.pdf" class="first">Paper</a> &bull; <a href="https://github.com/yangyi02/finegrained-pose">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/part/icon.png" alt="" />
      <p><strong><a href="">Recognizing Part Attributes with Insufficient Data</a></strong><br>
      Xiangyun Zhao, Yi Yang, Feng Zhou, Xiao Tan, Yuchen Yuan, Yingze Bao, Ying Wu<br>
      <em>ICCV 2019</em><br>
      <p class="link"><a href="https://openreview.net/pdf?id=jFuFNbdlWG" class="first">Paper</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/stereo_flow/icon.png" alt="" />
      <p><strong><a href="">UnOS: Unified Unsupervised Optical-flow and Stereo-depth Estimation by Watching Videos</a></strong><br>
      Yang Wang, Peng Wang, Zhenheng Yang, Chenxu Luo, Yi Yang, Wei Xu<br>
      <em>CVPR 2019</em><br>
      <p class="link"><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_UnOS_Unified_Unsupervised_Optical-Flow_and_Stereo-Depth_Estimation_by_Watching_Videos_CVPR_2019_paper.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/UnDepthflow">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose_dataset/icon.png" alt="" />
      <p><strong><a href="http://users.umiacs.umd.edu/~wym/3dpose.html">3D Pose Estimation for Fine-Grained Object Categories</a></strong><br>
      Yaming Wang, Xiao Tan, Yi Yang, Xiao Liu, Errui Ding, Feng Zhou, Larry S. Davis<br>
      <em>ECCV 2018 Workshop</em><br>
      <p class="link"><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Wang_3D_Pose_Estimation_for_Fine-Grained_Object_Categories_ECCVW_2018_paper.pdf" class="first">Paper</a> &bull; <a href="http://users.umiacs.umd.edu/~wym/3dpose.html">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/optical_flow/icon.png" alt="" />
      <p><strong><a href="research/optical_flow/optical_flow_cvpr2018.pdf">Occlusion Aware Unsupervised Learning of Optical Flow</a></strong><br>
      Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, Wei Xu<br>
      <em>CVPR 2018</em><br>
      <p class="link"><a href="research/optical_flow/optical_flow_cvpr2018.pdf" class="first">Paper</a> &bull; <a href="research/optical_flow/optical_flow_slides.pdf">Slides</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/feedback/icon.png" alt="" />
      <p><strong><a href="https://github.com/caochunshui/Feedback-CNN">Feedback Convolutional Neural Network for Visual Localization and Segmentation</a></strong><br>
      Chunshui Cao, Xianming Liu, Yi Yang, et al.<br>
      <em>PAMI 2018</em><br>
      <p class="link"><a href="research/feedback/feedback_pami2018.pdf" class="first">Paper</a> &bull; <a href="https://github.com/caochunshui/Feedback-CNN">Code</a> &bull; <a href="research/feedback/feedback_poster.pdf">Poster</a> &bull; <a href="research/feedback/feedback_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=kpN86noLAA4">Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/hand_depth/icon.png" alt="" />
      <p><strong><a href="https://github.com/jsupancic/deep_hand_pose">Depth-based Hand Pose Estimation: Data, Methods, and Challenges</a></strong><br>
      James Supancic, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan<br>
      <em>IJCV 2018</em><br>
      <p class="link"><a href="https://arxiv.org/pdf/1504.06378.pdf" class="first">Paper</a> &bull; <a href="https://github.com/jsupancic/deep_hand_pose">Code</a> &bull; <a href="http://arrummzen.net/#HandData">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/dynamic_time/icon.png" alt="" />
      <p><strong><a href="https://github.com/baidu-research/DT-RAM">Dynamic Computational Time for Visual Attention</a></strong><br>
      Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, Wei Xu<br>
      <em>ICCV 2017 Workshop</em><br>
      <p class="link"><a href="research/dynamic_time/dynamic_time_iccv2017w.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/DT-RAM">Code</a> &bull; <a href="research/dynamic_time/dynamic_time_slides.pdf">Slides</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/attention_scale/icon.png" alt="" />
      <p><strong><a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Attention to Scale: Scale-aware Semantic Image Segmentation</a></strong><br>
      Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L. Yuille<br>
      <em>CVPR 2016</em><br>
      <p class="link"><a href="research/attention_scale/attention_scale_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Project Page</a> &bull; <a href="research/attention_scale/attention_scale_slides.pdf">Slides</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/multi_label/icon.png" alt="" />
      <p><strong><a href="research/multi_label/multi_label_cvpr2016.pdf">CNN-RNN: A Unified Framework for Multi-label Image Classification</a></strong><br>
      Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu<br>
      <em>CVPR 2016</em><br>
      <p class="link"><a href="research/multi_label/multi_label_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="https://slideplayer.com/slide/12768107/">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=n1nTOAkTnIo">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/video_caption/icon.png" alt="" />
      <p><strong><a href="research/video_caption/video_caption_cvpr2016.pdf">Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</a></strong><br>
      Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu<br>
      <em>CVPR 2016</em><br>
      <p class="link"><a href="research/video_caption/video_caption_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="research/video_caption/video_caption_slides.pdf">Slides</a> &bull; <a href="research/video_caption/video_caption_poster.pdf">Poster</a> &bull; <a href="https://www.youtube.com/watch?v=QmLGgJTbtVU">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/densebox/icon.png" alt="" />
      <p><strong><a href="research/densebox/densebox_2015.pdf">DenseBox: Unifying Landmark Localization with End to End Object Detection</a></strong><br>
      Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu<br>
      <em>Arxiv 2015</em><br>
      <p class="link"><a href="research/densebox/densebox_2015.pdf" class="first">Paper</a> &bull; <a href="https://www.slideshare.net/ssuser2b0431/densebox">Slides</a> &bull; <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">Kitti Car Detection Benchmark</a> &bull; <a href="https://pan.baidu.com/s/1mgoWWsS">Demo Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/child_learn/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html">Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images</a></strong><br>
      Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille<br>
      <em>ICCV 2015</em><br>
      <p class="link"><a href="research/child_learn/child_learn_iccv2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/NVC-Dataset">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/feedback/icon.png" alt="" />
      <p><strong><a href="https://github.com/caochunshui/Feedback-CNN">Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks</a></strong><br>
      Chunshui Cao, Xianming Liu, Yi Yang, et al.<br>
      <em>ICCV 2015</em><br>
      <p class="link"><a href="research/feedback/feedback_iccv2015.pdf" class="first">Paper</a> &bull; <a href="https://github.com/caochunshui/Feedback-CNN">Code</a> &bull; <a href="research/feedback/feedback_slides.pdf">Slides</a> &bull; <a href="research/feedback/feedback_poster.pdf">Poster</a> &bull; <a href="https://www.youtube.com/watch?v=kpN86noLAA4">Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/hand_depth/icon.png" alt="" />
      <p><strong><a href="https://github.com/jsupancic/deep_hand_pose">Depth-based Hand Pose Estimation: Data, Methods, and Challenges</a></strong><br>
      James Supancic, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan<br>
      <em>ICCV 2015</em><br>
      <p class="link"><a href="research/hand_depth/hand_depth_iccv2015.pdf" class="first">Paper</a> &bull; <a href="https://github.com/jsupancic/deep_hand_pose">Code</a> &bull; <a href="http://arrummzen.net/#HandData">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/noisy_label/icon.png" alt="" />
      <p><strong><a href="http://github.com/Cysu/noisy_label">Learning from Massive Noisy Labeled Data for Image Classification</a></strong><br>
      Tong Xiao, Tian Xia, Yi Yang, Chang Huang, Xiaogang Wang<br>
      <em>CVPR 2015</em><br>
      <p class="link"><a href="research/noisy_label/noisy_label_cvpr2015.pdf" class="first">Paper</a> &bull; <a href="http://github.com/Cysu/noisy_label">Code</a> &bull; <a href="https://drive.google.com/drive/folders/0B67_d0rLRTQYU2E4aHNHaE1uMTg">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/deep_caption/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Deep Captioning with Multimodal Recurrent Neural Networks</a></strong><br>
      Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan Yuille<br>
      <em>ICLR 2015</em><br>
      <p class="link"><a href="research/deep_caption/deep_caption_iclr2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a> &bull; <a href="research/deep_caption/deep_caption_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=gr5N4aJ7fEg&feature=youtu.be">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/deep_caption/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Explain Images with Multimodal Recurrent Neural Networks</a></strong><br>
      Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan Yuille<br>
      <em>NIPS 2014 Workshop</em><br>
      <p class="link"><a href="research/deep_caption/explain_image_2014.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a> &bull; <a href="research/deep_caption/deep_caption_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=gr5N4aJ7fEg&feature=youtu.be">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/auto_caption/icon.png" alt="" />
      <p><strong><a href="research/auto_caption/auto_caption_wacv2014.pdf">AutoCaption: Automatic Caption Generation for Personal Photos</a></strong><br>
      Krishnan Ramnath, Simon Baker, et al.<br>
      <em>WACV 2014</em><br>
      <p class="link"><a href="research/auto_caption/auto_caption_wacv2014.pdf" class="first">Paper</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/parsing_occlude/icon.png" alt="" />
      <p><strong><a href="research/parsing_occlude/parsing_occlude_cvpr2014.pdf">Parsing Occluded People</a></strong><br>
      Golnaz Ghiasi, Yi Yang, Deva Ramanan, Charless Fowlkes<br>
      <em>CVPR 2014</em><br></p>
      <p class="link"><a href="research/parsing_occlude/parsing_occlude_cvpr2014.pdf" class="first">Paper</a> &bull; <a href="research/parsing_occlude/parsing_occlude_poster.pdf">Poster</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose/icon.jpg" alt="" />
      <p><strong><a href="research/pose/index.html">Articulated Human Detection with Flexible Mixtures of Parts</a></strong><br>
      Yi Yang, Deva Ramanan<br>
      <em>PAMI 2013</em><br>
      <p class="link"><a href="research/pose/pose_pami2013.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="research/pose/pose_slides.pdf">Slides</a> &bull; <a href="research/pose/pose_poster.pdf">Poster</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a> &bull; <a href="http://www.popsci.com/science/article/2012-09/deva-ramanan-trains-computers-identify-people">News</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/proxemics/icon.jpg" alt="" />
      <p><strong><a href="research/proxemics/proxemics_cvpr2012.pdf">Recognizing Proxemics in Personal Photos</a></strong><br>
      Yi Yang, Simon Baker, Anitha Kannan, Deva Ramanan<br>
      <em>CVPR 2012</em><br>
      <p class="link"><a href="research/proxemics/proxemics_cvpr2012.pdf" class="first">Paper</a> &bull; <a href="research/proxemics/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/proxemics_recognition">Code</a> &bull; <a href="research/proxemics/proxemics_slides.pdf">Slides</a> &bull; <a href="research/proxemics/proxemics_poster.pdf">Poster</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/layers/icon.jpg" alt="" />
      <p><strong><a href="research/layers/index.html">Layered Object Models for Image Segmentation</a></strong><br>
      Yi Yang, Sam Hallman, Deva Ramanan, Charless Fowlkes<br>
      <em>PAMI 2012</em><br>
      <p class="link"><a href="research/layers/layers_pami2012.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a> &bull; <a href="research/layers/layers_slides.pdf">Slides</a> &bull; <a href="research/layers/layers_poster.pdf">Poster</a> &bull; <a href="http://videolectures.net/cvpr2010_hallman_lodm/">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose/icon.jpg" alt="" />
      <p><strong><a href="research/pose/index.html">Articulated Pose Estimation with Flexible Mixtures of Parts</a></strong><br>
      Yi Yang, Deva Ramanan<br>
      <em>CVPR 2011</em><br>
      <p class="link"><a href="research/pose/pose_cvpr2011.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="research/pose/pose_slides.pdf">Slides</a> &bull; <a href="research/pose/pose_poster.pdf">Poster</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a> &bull; <a href="http://www.popsci.com/science/article/2012-09/deva-ramanan-trains-computers-identify-people">News</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/jccp/icon.jpg" alt="" />
      <p><strong><a href="research/jccp/jccp_or2011.pdf">Sequential Convex Approximations to Joint Chance Constrained Programs</a></strong><br>
      L. Jeff Hong, Yi Yang, Liwei Zhang<br>
      <em>OR 2011</em><br>
      <p class="link"><a href="research/jccp/jccp_or2011.pdf" class="first">Paper</a> &bull; <a href="https://github.com/yangyi02/sequential_chance_constrained">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/layers/icon.jpg" alt="" />
      <p><strong><a href="research/layers/index.html">Layered Object Detection for Multi-Class Segmentation</a></strong><br>
      Yi Yang, Sam Hallman, Deva Ramanan, Charless Fowlkes<br>
      <em>CVPR 2010</em><br>
      <p class="link"><a href="research/layers/layers_cvpr2010.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a> &bull; <a href="research/layers/layers_slides.pdf">Slides</a> &bull; <a href="research/layers/layers_poster.pdf">Poster</a> &bull; <a href="http://videolectures.net/cvpr2010_hallman_lodm/">Video Talk</a></p>
    </div>
  </div>

</body>
</html>
