<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZB7Y76KNGM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-ZB7Y76KNGM');
  </script>
  <title>Yi Yang's Homepage</title>
  <meta charset="utf-8">
  <link rel="stylesheet" type="text/css" href="style.css" media="all">
  <link rel='stylesheet' type='text/css' href='http://fonts.googleapis.com/css?family=Lato:300,400,700'>
</head>

<body>
  <div class="section">
    <div>
      <h1><span itemprop="name">Yi Yang (杨亿)</span></h1>
    </div>
    <p>I believe in Jesus Christ. 我信耶稣基督。</p>
    <p>I am currently a research scientist at <a href="http://www.deepmind.com/">Google DeepMind</a>. My research interests are self-supervised visual representation learning of motion, depth and segmentation for objects.</p>
    <p>Previously, I was a research scientist at <a href="http://research.baidu.com/">Baidu Research</a> from 2013 to 2018. 
      I obtained my Ph.D. degree in <a href="http://www.ics.uci.edu">Computer Science</a> at <a href="http://www.uci.edu">UC Irvine</a> in 2013. 
      I had summer internships at <a href="http://www.google.com/">Google</a> and <a href="http://research.microsoft.com/en-us/">Microsoft Research</a>. 
      I obtained my master degree in <a href="https://ieda.ust.hk/eng/index.php">Industrial Engineering</a> at <a href="http://www.ust.hk">Hong Kong University of Science and Technology</a> in 2008, and bachelor degree in <a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html">Automation</a> at <a href="http://www.tsinghua.edu.cn">Tsinghua University</a> in 2006.</p>
  </div>

  <div class="section">
    <h2>Publications</h2>

    <div class="publication">
      <img src="research/tapir/icon.png" alt="" />
      <p><strong><a href="https://deepmind-tapir.github.io/">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</a></strong><br>
      <em>ICCV 2023</em><br>
      <br>
      <p class="link"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf" class="first">Paper</a> &bull; <a href="https://deepmind-tapir.github.io/">Website</a> &bull; <a href="https://github.com/deepmind/tapnet">Dataset & Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/perception_test/icon.png" alt="" />
      <p><strong><a href="https://perception-test-challenge.github.io/">Perception Test: A Diagnostic Benchmark for Multimodal Video Models</a></strong><br>
      <em>NeurIPS 2023</em><br>
      <br>
      <p class="link"><a href="https://openreview.net/pdf?id=HYEGXFnPoq" class="first">Paper</a> &bull; <a href="https://github.com/deepmind/perception_test">Dataset & Code</a> &bull; <a href="https://www.deepmind.com/blog/measuring-perception-in-ai-models">Blog Post</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/tap_vid/icon.png" alt="" />
      <p><strong><a href="https://tapvid.github.io/">TAP-Vid: A Benchmark for Tracking Any Point in a Video</a></strong><br>
      <em>NeurIPS 2022</em><br>
      <br>
      <p class="link"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/58168e8a92994655d6da3939e7cc0918-Paper-Datasets_and_Benchmarks.pdf" class="first">Paper</a> &bull; <a href="https://tapvid.github.io/">Website</a> &bull;  <a href="https://github.com/deepmind/tapnet">Dataset & Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/lip_sync/icon.png" alt="" />
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">Large-Scale Multilingual Audio Video Dubbing</a></strong><br>
      <em>Arxiv 2020</em><br>
      <br>
      <p class="link"><a href="https://arxiv.org/pdf/2011.03530.pdf" class="first">Paper</a> &bull; <a href="https://www.youtube.com/channel/UC6CIb_eaKEsKE213TVEdYFw">Demo Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/refined_pose_dataset/icon.png" alt="" />
      <p><strong><a href="https://github.com/yangyi02/finegrained-pose">A Refined 3D Pose Dataset of Fine-Grained Object Categories</a></strong><br>
      <em>ICCV 2019 Workshop</em><br>
      <br>
      <p class="link"><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Wang_A_Refined_3D_Pose_Dataset_for_Fine-Grained_Object_Categories_ICCVW_2019_paper.pdf" class="first">Paper</a> &bull; <a href="https://github.com/yangyi02/finegrained-pose">Dataset & Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/part/icon.png" alt="" />
      <p><strong><a href="">Recognizing Part Attributes with Insufficient Data</a></strong><br>
      <em>ICCV 2019</em><br>
      <br>
      <p class="link"><a href="https://openreview.net/pdf?id=jFuFNbdlWG" class="first">Paper</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/stereo_flow/icon.png" alt="" />
      <p><strong><a href="">UnOS: Unified Unsupervised Optical-flow and Stereo-depth Estimation by Watching Videos</a></strong><br>
      <em>CVPR 2019</em><br>
      <br>
      <p class="link"><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_UnOS_Unified_Unsupervised_Optical-Flow_and_Stereo-Depth_Estimation_by_Watching_Videos_CVPR_2019_paper.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/UnDepthflow">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose_dataset/icon.png" alt="" />
      <p><strong><a href="http://users.umiacs.umd.edu/~wym/3dpose.html">3D Pose Estimation for Fine-Grained Object Categories</a></strong><br>
      <em>ECCV 2018 Workshop</em><br>
      <br>
      <p class="link"><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Wang_3D_Pose_Estimation_for_Fine-Grained_Object_Categories_ECCVW_2018_paper.pdf" class="first">Paper</a> &bull; <a href="http://users.umiacs.umd.edu/~wym/3dpose.html">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/optical_flow/icon.png" alt="" />
      <p><strong><a href="research/optical_flow/optical_flow_cvpr2018.pdf">Occlusion Aware Unsupervised Learning of Optical Flow</a></strong><br>
      <em>CVPR 2018</em><br>
      <br>
      <p class="link"><a href="research/optical_flow/optical_flow_cvpr2018.pdf" class="first">Paper</a> &bull; <a href="research/optical_flow/optical_flow_slides.pdf">Slides</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/feedback/icon.png" alt="" />
      <p><strong><a href="https://github.com/caochunshui/Feedback-CNN">Feedback Convolutional Neural Network for Visual Localization and Segmentation</a></strong><br>
      <em>PAMI 2018</em><br>
      <br>
      <p class="link"><a href="research/feedback/feedback_pami2018.pdf" class="first">Paper</a> &bull; <a href="https://github.com/caochunshui/Feedback-CNN">Code</a> &bull; <a href="research/feedback/feedback_poster.pdf">Poster</a> &bull; <a href="research/feedback/feedback_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=kpN86noLAA4">Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/hand_depth/icon.png" alt="" />
      <p><strong><a href="https://github.com/jsupancic/deep_hand_pose">Depth-based Hand Pose Estimation: Data, Methods, and Challenges</a></strong><br>
      <em>IJCV 2018</em><br>
      <br>
      <p class="link"><a href="https://arxiv.org/pdf/1504.06378.pdf" class="first">Paper</a> &bull; <a href="https://github.com/jsupancic/deep_hand_pose">Code</a> &bull; <a href="http://arrummzen.net/#HandData">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/dynamic_time/icon.png" alt="" />
      <p><strong><a href="https://github.com/baidu-research/DT-RAM">Dynamic Computational Time for Visual Attention</a></strong><br>
      <em>ICCV 2017 Workshop</em><br>
      <br>
      <p class="link"><a href="research/dynamic_time/dynamic_time_iccv2017w.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/DT-RAM">Code</a> &bull; <a href="research/dynamic_time/dynamic_time_slides.pdf">Slides</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/attention_scale/icon.png" alt="" />
      <p><strong><a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Attention to Scale: Scale-aware Semantic Image Segmentation</a></strong><br>
      <em>CVPR 2016</em><br>
      <br>
      <p class="link"><a href="research/attention_scale/attention_scale_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Project Page</a> &bull; <a href="research/attention_scale/attention_scale_slides.pdf">Slides</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/multi_label/icon.png" alt="" />
      <p><strong><a href="research/multi_label/multi_label_cvpr2016.pdf">CNN-RNN: A Unified Framework for Multi-label Image Classification</a></strong><br>
      <em>CVPR 2016</em><br>
      <br>
      <p class="link"><a href="research/multi_label/multi_label_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="https://slideplayer.com/slide/12768107/">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=n1nTOAkTnIo">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/video_caption/icon.png" alt="" />
      <p><strong><a href="research/video_caption/video_caption_cvpr2016.pdf">Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</a></strong><br>
      <em>CVPR 2016</em><br>
      <br>
      <p class="link"><a href="research/video_caption/video_caption_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="research/video_caption/video_caption_slides.pdf">Slides</a> &bull; <a href="research/video_caption/video_caption_poster.pdf">Poster</a> &bull; <a href="https://www.youtube.com/watch?v=QmLGgJTbtVU">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/densebox/icon.png" alt="" />
      <p><strong><a href="research/densebox/densebox_2015.pdf">DenseBox: Unifying Landmark Localization with End to End Object Detection</a></strong><br>
      <em>Arxiv 2015</em><br>
      <br>
      <p class="link"><a href="research/densebox/densebox_2015.pdf" class="first">Paper</a> &bull; <a href="https://www.slideshare.net/ssuser2b0431/densebox">Slides</a> &bull; <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">Kitti Car Detection Benchmark</a> &bull; <a href="https://pan.baidu.com/s/1mgoWWsS">Demo Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/child_learn/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html">Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images</a></strong><br>
      <em>ICCV 2015</em><br>
      <br>
      <p class="link"><a href="research/child_learn/child_learn_iccv2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/NVC-Dataset">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/feedback/icon.png" alt="" />
      <p><strong><a href="https://github.com/caochunshui/Feedback-CNN">Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks</a></strong><br>
      <em>ICCV 2015</em><br>
      <br>
      <p class="link"><a href="research/feedback/feedback_iccv2015.pdf" class="first">Paper</a> &bull; <a href="https://github.com/caochunshui/Feedback-CNN">Code</a> &bull; <a href="research/feedback/feedback_slides.pdf">Slides</a> &bull; <a href="research/feedback/feedback_poster.pdf">Poster</a> &bull; <a href="https://www.youtube.com/watch?v=kpN86noLAA4">Video</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/hand_depth/icon.png" alt="" />
      <p><strong><a href="https://github.com/jsupancic/deep_hand_pose">Depth-based Hand Pose Estimation: Data, Methods, and Challenges</a></strong><br>
      <em>ICCV 2015</em><br>
      <br>
      <p class="link"><a href="research/hand_depth/hand_depth_iccv2015.pdf" class="first">Paper</a> &bull; <a href="https://github.com/jsupancic/deep_hand_pose">Code</a> &bull; <a href="http://arrummzen.net/#HandData">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/noisy_label/icon.png" alt="" />
      <p><strong><a href="http://github.com/Cysu/noisy_label">Learning from Massive Noisy Labeled Data for Image Classification</a></strong><br>
      <em>CVPR 2015</em><br>
      <br>
      <p class="link"><a href="research/noisy_label/noisy_label_cvpr2015.pdf" class="first">Paper</a> &bull; <a href="http://github.com/Cysu/noisy_label">Code</a> &bull; <a href="https://drive.google.com/drive/folders/0B67_d0rLRTQYU2E4aHNHaE1uMTg">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/deep_caption/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Deep Captioning with Multimodal Recurrent Neural Networks</a></strong><br>
      <em>ICLR 2015</em><br>
      <br>
      <p class="link"><a href="research/deep_caption/deep_caption_iclr2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a> &bull; <a href="research/deep_caption/deep_caption_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=gr5N4aJ7fEg&feature=youtu.be">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/deep_caption/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Explain Images with Multimodal Recurrent Neural Networks</a></strong><br>
      <em>NIPS 2014 Workshop</em><br>
      <br>
      <p class="link"><a href="research/deep_caption/explain_image_2014.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a> &bull; <a href="research/deep_caption/deep_caption_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=gr5N4aJ7fEg&feature=youtu.be">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/auto_caption/icon.png" alt="" />
      <p><strong><a href="research/auto_caption/auto_caption_wacv2014.pdf">AutoCaption: Automatic Caption Generation for Personal Photos</a></strong><br>
      <em>WACV 2014</em><br>
      <br>
      <p class="link"><a href="research/auto_caption/auto_caption_wacv2014.pdf" class="first">Paper</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/parsing_occlude/icon.png" alt="" />
      <p><strong><a href="research/parsing_occlude/parsing_occlude_cvpr2014.pdf">Parsing Occluded People</a></strong><br>
      <em>CVPR 2014</em><br></p>
      <br>
      <p class="link"><a href="research/parsing_occlude/parsing_occlude_cvpr2014.pdf" class="first">Paper</a> &bull; <a href="research/parsing_occlude/parsing_occlude_poster.pdf">Poster</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose/icon.jpg" alt="" />
      <p><strong><a href="research/pose/index.html">Articulated Human Detection with Flexible Mixtures of Parts</a></strong><br>
      <em>PAMI 2013</em><br>
      <br>
      <p class="link"><a href="research/pose/pose_pami2013.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="research/pose/pose_slides.pdf">Slides</a> &bull; <a href="research/pose/pose_poster.pdf">Poster</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a> &bull; <a href="http://www.popsci.com/science/article/2012-09/deva-ramanan-trains-computers-identify-people">News</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/proxemics/icon.jpg" alt="" />
      <p><strong><a href="research/proxemics/proxemics_cvpr2012.pdf">Recognizing Proxemics in Personal Photos</a></strong><br>
      <em>CVPR 2012</em><br>
      <br>
      <p class="link"><a href="research/proxemics/proxemics_cvpr2012.pdf" class="first">Paper</a> &bull; <a href="research/proxemics/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/proxemics_recognition">Code</a> &bull; <a href="research/proxemics/proxemics_slides.pdf">Slides</a> &bull; <a href="research/proxemics/proxemics_poster.pdf">Poster</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/layers/icon.jpg" alt="" />
      <p><strong><a href="research/layers/index.html">Layered Object Models for Image Segmentation</a></strong><br>
      <em>PAMI 2012</em><br>
      <br>
      <p class="link"><a href="research/layers/layers_pami2012.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a> &bull; <a href="research/layers/layers_slides.pdf">Slides</a> &bull; <a href="research/layers/layers_poster.pdf">Poster</a> &bull; <a href="http://videolectures.net/cvpr2010_hallman_lodm/">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose/icon.jpg" alt="" />
      <p><strong><a href="research/pose/index.html">Articulated Pose Estimation with Flexible Mixtures of Parts</a></strong><br>
      <em>CVPR 2011</em><br>
      <br>
      <p class="link"><a href="research/pose/pose_cvpr2011.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="research/pose/pose_slides.pdf">Slides</a> &bull; <a href="research/pose/pose_poster.pdf">Poster</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a> &bull; <a href="http://www.popsci.com/science/article/2012-09/deva-ramanan-trains-computers-identify-people">News</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/jccp/icon.jpg" alt="" />
      <p><strong><a href="research/jccp/jccp_or2011.pdf">Sequential Convex Approximations to Joint Chance Constrained Programs</a></strong><br>
      <em>OR 2011</em><br>
      <br>
      <p class="link"><a href="research/jccp/jccp_or2011.pdf" class="first">Paper</a> &bull; <a href="https://github.com/yangyi02/sequential_chance_constrained">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/layers/icon.jpg" alt="" />
      <p><strong><a href="research/layers/index.html">Layered Object Detection for Multi-Class Segmentation</a></strong><br>
      <em>CVPR 2010</em><br>
      <br>
      <p class="link"><a href="research/layers/layers_cvpr2010.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a> &bull; <a href="research/layers/layers_slides.pdf">Slides</a> &bull; <a href="research/layers/layers_poster.pdf">Poster</a> &bull; <a href="http://videolectures.net/cvpr2010_hallman_lodm/">Video Talk</a></p>
    </div>
  </div>

</body>
</html>
