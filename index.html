<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZB7Y76KNGM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-ZB7Y76KNGM');
  </script>
  <title>Yi Yang's Homepage</title>
  <meta charset="utf-8">
  <link rel="stylesheet" type="text/css" href="style.css" media="all">
  <link rel='stylesheet' type='text/css' href='http://fonts.googleapis.com/css?family=Lato:300,400,700'>
</head>

<body>
  <div class="section">
    <!-- 左边文字 -->
    <div style="display: table-cell; vertical-align: middle; padding-right: 20px;">
      <h1><span itemprop="name">Yi Yang (杨亿)</span></h1>
      <p>I believe solely in Jesus. 我独信耶稣。</p>
      <p>I am currently a Research Scientist at Google DeepMind, where my research focuses on self-supervised visual representation learning for robotics.</p>
      <p>Previously, I was a Research Scientist at Baidu Research from 2013 to 2018. 
        I received my Ph.D. from UC Irvine in 2013 under the supervision of Prof. Deva Ramanan. 
        During my graduate studies, I completed summer internships at Microsoft Research (2011) and Google (2012). 
        I finished my master study at the Hong Kong University of Science and Technology in 2008 and bachelor study at Tsinghua University in 2006.</p>    
      <p>Full publications: <a href="https://scholar.google.com/citations?user=-BO7TXUAAAAJ">Google Scholar</a></p>
    </div>

    <!-- 右边图片 -->
    <div style="display: table-cell; vertical-align: middle; width: 200px;">
      <img src="yang_yi.jpg" id="title-photo" itemprop="photo" alt="Yi Yang" style="width: 100%; border-radius: 10px;">
    </div>
  </div>

  <div class="section">
    <h2>Selected Publications</h2>

    <div class="publication">
      <img src="research/tapnext/icon.png" alt="" />
      <p><strong><a href="https://tap-next.github.io/">TAPNext: Tracking Any Point (TAP) as Next Token Prediction</a></strong><br>
      <em>ICCV 2025</em><br>
      <br>
      <p class="link"><a href="https://arxiv.org/pdf/2504.05579" class="first">Paper</a> &bull; <a href="https://tap-next.github.io/">Project Page</a> &bull; <a href="https://github.com/deepmind/tapnet">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/tapvid3d/icon.png" alt="" />
      <p><strong><a href="https://tapvid3d.github.io/">TAPVid-3D: A Benchmark for Tracking Any Point in 3D</a></strong><br>
      <em>NeurIPS 2024</em><br>
      <br>
      <p class="link"><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9566607d423f8c32a2d5ce09a8b62232-Paper-Datasets_and_Benchmarks_Track.pdf" class="first">Paper</a> &bull; <a href="https://tapvid3d.github.io/">Project Page</a> &bull; <a href="https://github.com/deepmind/tapnet">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/robotap/icon.png" alt="" />
      <p><strong><a href="https://robotap.github.io/">RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation</a></strong><br>
      <em>ICRA 2024</em><br>
      <br>
      <p class="link"><a href="https://discovery.ucl.ac.uk/id/eprint/10196975/1/robotapicra24.pdf" class="first">Paper</a> &bull; <a href="https://robotap.github.io/">Project Page</a> &bull; <a href="https://github.com/deepmind/tapnet">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/tapir/icon.png" alt="" />
      <p><strong><a href="https://deepmind-tapir.github.io/">TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement</a></strong><br>
      <em>ICCV 2023</em><br>
      <br>
      <p class="link"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf" class="first">Paper</a> &bull; <a href="https://deepmind-tapir.github.io/">Project Page</a> &bull; <a href="https://github.com/deepmind/tapnet">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/tap_vid/icon.png" alt="" />
      <p><strong><a href="https://tapvid.github.io/">TAP-Vid: A Benchmark for Tracking Any Point in a Video</a></strong><br>
      <em>NeurIPS 2022</em><br>
      <br>
      <p class="link"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/58168e8a92994655d6da3939e7cc0918-Paper-Datasets_and_Benchmarks.pdf" class="first">Paper</a> &bull; <a href="https://tapvid.github.io/">Project Page</a> &bull;  <a href="https://github.com/deepmind/tapnet">Dataset</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/stereo_flow/icon.png" alt="" />
      <p><strong><a href="">UnOS: Unified Unsupervised Optical-flow and Stereo-depth Estimation by Watching Videos</a></strong><br>
      <em>CVPR 2019</em><br>
      <br>
      <p class="link"><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_UnOS_Unified_Unsupervised_Optical-Flow_and_Stereo-Depth_Estimation_by_Watching_Videos_CVPR_2019_paper.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/UnDepthflow">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/optical_flow/icon.png" alt="" />
      <p><strong><a href="research/optical_flow/optical_flow_cvpr2018.pdf">Occlusion Aware Unsupervised Learning of Optical Flow</a></strong><br>
      <em>CVPR 2018</em><br>
      <br>
      <p class="link"><a href="research/optical_flow/optical_flow_cvpr2018.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/UnDepthflow">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/attention_scale/icon.png" alt="" />
      <p><strong><a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Attention to Scale: Scale-aware Semantic Image Segmentation</a></strong><br>
      <em>CVPR 2016</em><br>
      <br>
      <p class="link"><a href="research/attention_scale/attention_scale_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Project Page</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/deep_caption/icon.png" alt="" />
      <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Deep Captioning with Multimodal Recurrent Neural Networks</a></strong><br>
      <em>ICLR 2015</em><br>
      <br>
      <p class="link"><a href="research/deep_caption/deep_caption_iclr2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/proxemics/icon.jpg" alt="" />
      <p><strong><a href="research/proxemics/proxemics_cvpr2012.pdf">Recognizing Proxemics in Personal Photos</a></strong><br>
      <em>CVPR 2012</em><br>
      <br>
      <p class="link"><a href="research/proxemics/proxemics_cvpr2012.pdf" class="first">Paper</a> &bull; <a href="research/proxemics/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/proxemics_recognition">Code</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/pose/icon.jpg" alt="" />
      <p><strong><a href="research/pose/index.html">Articulated Pose Estimation with Flexible Mixtures of Parts</a></strong><br>
      <em>CVPR 2011</em><br>
      <br>
      <p class="link"><a href="research/pose/pose_cvpr2011.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a></p>
    </div>
    <div class="line"></div>

    <div class="publication">
      <img src="research/layers/icon.jpg" alt="" />
      <p><strong><a href="research/layers/index.html">Layered Object Detection for Multi-Class Segmentation</a></strong><br>
      <em>CVPR 2010</em><br>
      <br>
      <p class="link"><a href="research/layers/layers_cvpr2010.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a></p>
    </div>
  </div>

</body>
</html>
