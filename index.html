<!DOCTYPE html>
<html lang="en">
<head>
	<title>Yi Yang - UC Irvine</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="styles/style.css" media="all">
	<link rel='stylesheet' type='text/css' href='http://fonts.googleapis.com/css?family=Lato:300,400,700'>
</head>

<body>
	<div class="section">
		<div> <img src="me/yang_yi.jpg" id="title-photo" itemprop="photo" alt="" /> </div>
		<div>
			<h1><span itemprop="name">Yi Yang (杨亿)</span></h1>
			<p>Research Scientist<br>
			Baidu Research<br>
      1195 Bordeaux Drive, Sunnyvale, CA<br>
      Email: <a href="mailto:yangyi05@baidu.com">yangyi05@baidu.com</a></p>
      <p class="link"><a href="resume/cv.pdf">Resume</a> &bull; <a href="https://scholar.google.com/citations?user=-BO7TXUAAAAJ&hl=en">Scholar</a> &bull; <a href="https://github.com/yangyi02">Github</a> &bull; <a href="https://www.linkedin.com/profile/preview?locale=en_US&trk=prof-0-sb-preview-primary-button">Linkedin</a></p>
    </div>
  </div>

	<div class="section">
		<h2>About Me</h2>
    <p>I am a big fan of <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a>. I believe that one day robots will help the human in all our daily lives.</p>
    <p>I am currently a research scientist at <a href="http://research.baidu.com/">Baidu Research, Institute of Deep Learning</a>. My research focuses on developing algorithms that facilitate machines to learn as effectively as human, through self-supervised learning and multi-modal learning, with the target on high level visual understanding tasks.</p>
    <p>Previously, I had a lot of fun collabrating with <a href="https://scholar.google.com/citations?user=Gxz1fqwAAAAJ&hl=en">Wei Xu</a>, <a href="http://users.eecs.northwestern.edu/~jwa368/">Jiang Wang</a>, <a href="https://scholar.google.com/citations?user=IyyEKyIAAAAJ&hl=en">Chang Huang</a>, <a href="https://scholar.google.com/citations?user=y5zkBeMAAAAJ&hl=en">Kai Yu</a> and <a href="http://www.andrewng.org/">Prof. Andrew Ng</a> on building supervised deep learning models for computer vision tasks including Object Detection, Semantic Image Segmentation, Visual Attention Models, Image Captioning, Visual Question Anwswering, Feedback Neural Networks, et. al.</p>
    <p>I obtained my Ph.D. degree in <a href="http://www.ics.uci.edu">Computer Science</a> at <a href="http://www.uci.edu">UC Irvine</a> in 2013. I was a member of the <a href="http://vision.ics.uci.edu">Computational Vision Group</a> where I was advised by <a href="http://www.cs.cmu.edu/~deva/">Prof. Deva Ramanan</a> (co-advised by <a href="http://www.ics.uci.edu/~fowlkes/">Prof. Charless Fowlkes</a>). My Ph.D. research focuses on High-Level Structured Image Understanding, including <a href="https://en.wikipedia.org/wiki/Object_detection">Object Detection</a>, <a href="https://en.wikipedia.org/wiki/Image_segmentation">Semantic Image Segmentation</a> and <a href="https://en.wikipedia.org/wiki/Articulated_body_pose_estimation">Articulated Human Pose Estimation</a>.</p>
    <p>I had summer internships at <a href="http://www.google.com/">Google</a> and <a href="http://research.microsoft.com/en-us/">Microsoft Research</a>. At Google, I worked with <a href="https://geometry.stanford.edu/person.php?id=eantunez">Emilio Antunez</a> and <a href="https://www.linkedin.com/in/tianliyu">Tianli Yu</a> on <a href="https://googleblog.blogspot.com/2012/11/deck-halls-with-tools-for-shopping.html">Visual Clothing Search</a>. At Microsoft Research, I worked with <a href="https://scholar.google.com/citations?user=UcbjgQ0AAAAJ&hl=en">Simon Baker</a> and <a href="https://scholar.google.com/citations?user=eoBHpj4AAAAJ&hl=en">Anitha Kannan</a> on <a href="https://en.wikipedia.org/wiki/Proxemics">Proxemics Recognition</a>. I completed a master degree in <a href="http://www.ielm.ust.hk">Industrial Engineering</a> at <a href="http://www.ust.hk">Hong Kong University of Science and Technology</a> with <a href="http://personal.cb.cityu.edu.hk/jeffhong/">Prof. L. Jeff Hong</a>, and a bachelor degree in <a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html">Automation</a> at <a href="http://www.tsinghua.edu.cn">Tsinghua University</a>.</p>
  </div>

	<div class="section">
		<h2>Publications</h2>
		
		<div class="publication">
			<img src="research/optical_flow/optical_flow.png" alt="" />
		    <p><strong><a href="research/optical_flow/optical_flow_cvpr2018.pdf">Occlusion Aware Unsupervised Learning of Optical Flow</a></strong><br>
		    Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, Wei Xu<br>
		    <em>CVPR 2018</em><br>
        <p class="link"><a href="research/optical_flow/optical_flow_cvpr2018.pdf" class="first">Paper</a> &bull; <a href="research/optical_flow/optical_flow_slides.pdf">Slides</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/feedback/feedback.png" alt="" />
		    <p><strong><a href="https://github.com/caochunshui/Feedback-CNN">Feedback Convolutional Neural Network for Visual Localization and Segmentation</a></strong><br>
		    Chunshui Cao, Xianming Liu, Yi Yang, et al.<br>
		    <em>PAMI 2018</em><br>
		    <p class="link"><a href="research/feedback/feedback_pami2018.pdf" class="first">Paper</a> &bull; <a href="https://github.com/caochunshui/Feedback-CNN">Code</a> &bull; <a href="research/feedback/feedback_poster.pdf">Poster</a> &bull; <a href="research/feedback/feedback_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=kpN86noLAA4">Video</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/hand_depth/hand_depth.png" alt="" />
		    <p><strong><a href="https://github.com/jsupancic/deep_hand_pose">Depth-based Hand Pose Estimation: Data, Methods, and Challenges</a></strong><br>
		    James Supancic, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan<br>
		    <em>IJCV 2018</em><br>
		    <p class="link"><a href="https://arxiv.org/pdf/1504.06378.pdf" class="first">Paper</a> &bull; <a href="https://github.com/jsupancic/deep_hand_pose">Code</a> &bull; <a href="http://arrummzen.net/#HandData">Dataset</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/dynamic_time/dynamic_time.png" alt="" />
		    <p><strong><a href="https://github.com/baidu-research/DT-RAM">Dynamic Computational Time for Visual Attention</a></strong><br>
		    Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, Wei Xu<br>
		    <em>ICCV 2017 Workshop</em><br>
		    <p class="link"><a href="research/dynamic_time/dynamic_time_iccv2017w.pdf" class="first">Paper</a> &bull; <a href="https://github.com/baidu-research/DT-RAM">Code</a> &bull; <a href="research/dynamic_time/dynamic_time_slides.pdf">Slides</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/attention_scale/attention_scale.png" alt="" />
		    <p><strong><a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Attention to Scale: Scale-aware Semantic Image Segmentation</a></strong><br>
		    Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L. Yuille<br>
		    <em>CVPR 2016</em><br>
        <p class="link"><a href="research/attention_scale/attention_scale_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="http://liangchiehchen.com/projects/DeepLab.html#attention model">Project Page</a> &bull; <a href="research/attention_scale/attention_scale_slides.pdf">Slides</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/multi_label/multi_label.png" alt="" />
		    <p><strong><a href="research/multi_label/multi_label_cvpr2016.pdf">CNN-RNN: A Unified Framework for Multi-label Image Classification</a></strong><br>
		    Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu<br>
		    <em>CVPR 2016</em><br>
        <p class="link"><a href="research/multi_label/multi_label_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="https://slideplayer.com/slide/12768107/">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=n1nTOAkTnIo">Video Talk</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/video_caption/video_caption.png" alt="" />
		    <p><strong><a href="research/video_caption/video_caption_cvpr2016.pdf">Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</a></strong><br>
		    Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu<br>
		    <em>CVPR 2016</em><br>
		    <p class="link"><a href="research/video_caption/video_caption_cvpr2016.pdf" class="first">Paper</a> &bull; <a href="research/video_caption/video_caption_slides.pdf">Slides</a> &bull; <a href="research/video_caption/video_caption_poster.pdf">Poster</a> &bull; <a href="https://www.youtube.com/watch?v=QmLGgJTbtVU">Video Talk</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/densebox/densebox.png" alt="" />
		    <p><strong><a href="research/densebox/densebox_2015.pdf">DenseBox: Unifying Landmark Localization with End to End Object Detection</a></strong><br>
		    Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu<br>
		    <em>Arxiv 2015</em><br>
        <p class="link"><a href="research/densebox/densebox_2015.pdf" class="first">Paper</a> &bull; <a href="https://www.slideshare.net/ssuser2b0431/densebox">Slides</a> &bull; <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php">Kitti Car Detection Benchmark</a> &bull; <a href="https://pan.baidu.com/s/1mgoWWsS">Demo Video</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/child_learn/child_learn.png" alt="" />
		    <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html">Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images</a></strong><br>
		    Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille<br>
		    <em>ICCV 2015</em><br>
		    <p class="link"><a href="research/child_learn/child_learn_iccv2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/NVC-Dataset">Dataset</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/feedback/feedback.png" alt="" />
		    <p><strong><a href="https://github.com/caochunshui/Feedback-CNN">Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks</a></strong><br>
		    Chunshui Cao, Xianming Liu, Yi Yang, et al.<br>
		    <em>ICCV 2015</em><br>
		    <p class="link"><a href="research/feedback/feedback_iccv2015.pdf" class="first">Paper</a> &bull; <a href="https://github.com/caochunshui/Feedback-CNN">Code</a> &bull; <a href="research/feedback/feedback_slides.pdf">Slides</a> &bull; <a href="research/feedback/feedback_poster.pdf">Poster</a> &bull; <a href="https://www.youtube.com/watch?v=kpN86noLAA4">Video</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/hand_depth/hand_depth.png" alt="" />
		    <p><strong><a href="https://github.com/jsupancic/deep_hand_pose">Depth-based Hand Pose Estimation: Data, Methods, and Challenges</a></strong><br>
		    James Supancic, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan<br>
		    <em>ICCV 2015</em><br>
		    <p class="link"><a href="research/hand_depth/hand_depth_iccv2015.pdf" class="first">Paper</a> &bull; <a href="https://github.com/jsupancic/deep_hand_pose">Code</a> &bull; <a href="http://arrummzen.net/#HandData">Dataset</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/noisy_label/noisy_label.png" alt="" />
		    <p><strong><a href="http://github.com/Cysu/noisy_label">Learning from Massive Noisy Labeled Data for Image Classification</a></strong><br>
		    Tong Xiao, Tian Xia, Yi Yang, Chang Huang, Xiaogang Wang<br>
		    <em>CVPR 2015</em><br>
        <p class="link"><a href="research/noisy_label/noisy_label_cvpr2015.pdf" class="first">Paper</a> &bull; <a href="http://github.com/Cysu/noisy_label">Code</a> &bull; <a href="https://drive.google.com/drive/folders/0B67_d0rLRTQYU2E4aHNHaE1uMTg">Dataset</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/deep_caption/deep_caption.png" alt="" />
		    <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Deep Captioning with Multimodal Recurrent Neural Networks</a></strong><br>
		    Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan Yuille<br>
		    <em>ICLR 2015</em><br>
		    <p class="link"><a href="research/deep_caption/deep_caption_iclr2015.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a> &bull; <a href="research/deep_caption/deep_caption_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=gr5N4aJ7fEg&feature=youtu.be">Video Talk</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/deep_caption/deep_caption.png" alt="" />
		    <p><strong><a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Explain Images with Multimodal Recurrent Neural Networks</a></strong><br>
		    Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan Yuille<br>
		    <em>NIPS 2014 Workshop</em><br>
        <p class="link"><a href="research/deep_caption/explain_image_2014.pdf" class="first">Paper</a> &bull; <a href="http://www.stat.ucla.edu/~junhua.mao/m-RNN.html">Project Page</a> &bull; <a href="https://github.com/mjhucla/TF-mRNN">Code</a> &bull; <a href="research/deep_caption/deep_caption_slides.pdf">Slides</a> &bull; <a href="https://www.youtube.com/watch?v=gr5N4aJ7fEg&feature=youtu.be">Video Talk</a></p>
		</div>
		<div class="line"></div>

		<div class="publication">
		    <img src="research/auto_caption/auto_caption.png" alt="" />
		    <p><strong><a href="research/auto_caption/auto_caption_wacv2014.pdf">AutoCaption: Automatic Caption Generation for Personal Photos</a></strong><br>
		    Krishnan Ramnath, Simon Baker, et al.<br>
		    <em>WACV 2014</em><br>
		    <p class="link"><a href="research/auto_caption/auto_caption_wacv2014.pdf" class="first">Paper</a></p>
		</div>
		<div class="line"></div>

	    <div class="publication">
		    <img src="research/parsing_occlude/parsing_occlude.png" alt="" />
		    <p><strong><a href="research/parsing_occlude/parsing_occlude_cvpr2014.pdf">Parsing Occluded People</a></strong><br>
		    Golnaz Ghiasi, Yi Yang, Deva Ramanan, Charless Fowlkes<br>
		    <em>CVPR 2014</em><br></p>
		    <p class="link"><a href="research/parsing_occlude/parsing_occlude_cvpr2014.pdf" class="first">Paper</a> &bull; <a href="research/parsing_occlude/parsing_occlude_poster.pdf">Poster</a></p>
	    </div>
	    <div class="line"></div>

        <div class="publication">
		    <img src="research/pose/pose.jpg" alt="" />
		    <p><strong><a href="research/pose/index.html">Articulated Human Detection with Flexible Mixtures of Parts</a></strong><br> 
		    Yi Yang, Deva Ramanan<br>
		    <em>PAMI 2013</em><br>
		    <p class="link"><a href="research/pose/pose_pami2013.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="research/pose/pose_slides.pdf">Slides</a> &bull; <a href="research/pose/pose_poster.pdf">Poster</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a> &bull; <a href="http://www.popsci.com/science/article/2012-09/deva-ramanan-trains-computers-identify-people">News</a></p>
	    </div>
	    <div class="line"></div>

	    <div class="publication">
		    <img src="research/proxemics/proxemics.jpg" alt="" />
		    <p><strong><a href="research/proxemics/proxemics_cvpr2012.pdf">Recognizing Proxemics in Personal Photos</a></strong><br>
		    Yi Yang, Simon Baker, Anitha Kannan, Deva Ramanan<br>
		    <em>CVPR 2012</em><br>
		    <p class="link"><a href="research/proxemics/proxemics_cvpr2012.pdf" class="first">Paper</a> &bull; <a href="research/proxemics/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/proxemics_recognition">Code</a> &bull; <a href="research/proxemics/proxemics_slides.pdf">Slides</a> &bull; <a href="research/proxemics/proxemics_poster.pdf">Poster</a></p>
		</div>
		<div class="line"></div>

	    <div class="publication">
		    <img src="research/layers/layers.jpg" alt="" />
		    <p><strong><a href="research/layers/index.html">Layered Object Models for Image Segmentation</a></strong><br>
		    Yi Yang, Sam Hallman, Deva Ramanan, Charless Fowlkes<br>
		    <em>PAMI 2012</em><br>
		    <p class="link"><a href="research/layers/layers_pami2012.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a> &bull; <a href="research/layers/layers_slides.pdf">Slides</a> &bull; <a href="research/layers/layers_poster.pdf">Poster</a> &bull; <a href="http://videolectures.net/cvpr2010_hallman_lodm/">Video Talk</a></p>
		</div>
		<div class="line"></div>

	    <div class="publication">
		    <img src="research/pose/pose.jpg" alt="" />
		    <p><strong><a href="research/pose/index.html">Articulated Pose Estimation with Flexible Mixtures of Parts</a></strong><br>
		    Yi Yang, Deva Ramanan<br>
		    <em>CVPR 2011</em><br>
		    <p class="link"><a href="research/pose/pose_cvpr2011.pdf" class="first">Paper</a> &bull; <a href="research/pose/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/pose_estimation">Code</a> &bull; <a href="research/pose/pose_slides.pdf">Slides</a> &bull; <a href="research/pose/pose_poster.pdf">Poster</a> &bull; <a href="http://techtalks.tv/talks/articulated-pose-estimation-with-flexible-mixtures-of-parts/54210/">Video Talk</a> &bull; <a href="http://www.popsci.com/science/article/2012-09/deva-ramanan-trains-computers-identify-people">News</a></p>
		</div>
		<div class="line"></div>

	    <div class="publication">
		    <img src="research/jccp/jccp.jpg" alt="" />
		    <p><strong><a href="research/jccp/jccp_or2011.pdf">Sequential Convex Approximations to Joint Chance Constrained Programs</a></strong><br>
		    L. Jeff Hong, Yi Yang, Liwei Zhang<br>
		    <em>OR 2011</em><br>
		    <p class="link"><a href="research/jccp/jccp_or2011.pdf" class="first">Paper</a></p>
		</div>
		<div class="line"></div>

	    <div class="publication">
		    <img src="research/layers/layers.jpg" alt="" />
		    <p><strong><a href="research/layers/index.html">Layered Object Detection for Multi-Class Segmentation</a></strong><br>
		    Yi Yang, Sam Hallman, Deva Ramanan, Charless Fowlkes<br>
		    <em>CVPR 2010</em><br>
		    <p class="link"><a href="research/layers/layers_cvpr2010.pdf" class="first">Paper</a> &bull; <a href="research/layers/index.html">Project Page</a> &bull; <a href="https://github.com/yangyi02/layered_segmentation">Code</a> &bull; <a href="research/layers/layers_slides.pdf">Slides</a> &bull; <a href="research/layers/layers_poster.pdf">Poster</a> &bull; <a href="http://videolectures.net/cvpr2010_hallman_lodm/">Video Talk</a></p>
		</div>
	</div>
	<!--
	<div class="section">
		<h2>Extra</h2>
		<p class="link"><a href="extra/ai_giants.html" class="first">Giants in Machine Learning and Computer Vision</a></p>
  	</div>
  	-->
</body>
</html>
